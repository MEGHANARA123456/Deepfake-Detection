{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b536598",
   "metadata": {},
   "source": [
    "# Deepfake Detection — Colab-Ready Notebook (with Grad-CAM)\n",
    "\n",
    "**Author:** Your Name\n",
    "\n",
    "This Colab-ready notebook downloads the Kaggle dataset `xdxd003/ff-c23`, extracts fixed frames (10 per video) into `data/real/` and `data/fake/`, trains a transfer-learning model, evaluates it, and shows Grad-CAM visualizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b05554",
   "metadata": {},
   "source": [
    "## 0 — Runtime & Setup (Colab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b4d431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run in Colab)\n",
    "!pip install -q kaggle mtcnn tensorflow==2.11.0 opencv-python-headless==4.6.0.66 scikit-learn matplotlib pillow\n",
    "\n",
    "import os\n",
    "os.makedirs('data/real', exist_ok=True)\n",
    "os.makedirs('data/fake', exist_ok=True)\n",
    "print('Setup complete. Upload kaggle.json to the runtime or place it in ~/.kaggle/kaggle.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536dda4e",
   "metadata": {},
   "source": [
    "## 1 — Download Kaggle dataset `xdxd003/ff-c23`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26e368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset using Kaggle API (Colab)\n",
    "import os, shutil\n",
    "kaggle_json_path = '/root/.kaggle/kaggle.json'\n",
    "if os.path.exists('kaggle.json') and not os.path.exists(kaggle_json_path):\n",
    "    os.makedirs('/root/.kaggle', exist_ok=True)\n",
    "    shutil.move('kaggle.json', kaggle_json_path)\n",
    "    os.chmod(kaggle_json_path, 0o600)\n",
    "\n",
    "dataset_ref = 'xdxd003/ff-c23'\n",
    "download_dir = 'kaggle_dataset'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "try:\n",
    "    !kaggle datasets download -d {dataset_ref} -p {download_dir} --unzip\n",
    "    print('Downloaded dataset to', download_dir)\n",
    "except Exception as e:\n",
    "    print('Error downloading via kaggle API. Ensure kaggle.json is uploaded. Error:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c4cdb3",
   "metadata": {},
   "source": [
    "## 2 — Inspect dataset structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b50683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inspect of dataset\n",
    "import os\n",
    "dataset_root = 'kaggle_dataset'\n",
    "for root, dirs, files in os.walk(dataset_root):\n",
    "    depth = root[len(dataset_root):].count(os.sep)\n",
    "    if depth <= 2:\n",
    "        print(root, '->', len(files), 'files, dirs:', dirs[:6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa8478c",
   "metadata": {},
   "source": [
    "## 3 — Extract fixed frames per video into data/real and data/fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a15a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2, numpy as np, os\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "detector = MTCNN()\n",
    "OUT_REAL = 'data/real'\n",
    "OUT_FAKE = 'data/fake'\n",
    "os.makedirs(OUT_REAL, exist_ok=True)\n",
    "os.makedirs(OUT_FAKE, exist_ok=True)\n",
    "\n",
    "def save_face_from_frame(img, out_path, target_size=(224,224)):\n",
    "    if img is None:\n",
    "        return False\n",
    "    if isinstance(img, np.ndarray):\n",
    "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        pil = Image.fromarray(rgb)\n",
    "    else:\n",
    "        pil = img.convert('RGB')\n",
    "    try:\n",
    "        boxes = detector.detect(pil)\n",
    "        if boxes is not None and len(boxes[0])>0:\n",
    "            x1, y1, x2, y2 = boxes[0]\n",
    "            face = pil.crop((int(x1), int(y1), int(x2), int(y2)))\n",
    "        else:\n",
    "            w, h = pil.size; s = min(w,h); left=(w-s)//2; top=(h-s)//2\n",
    "            face = pil.crop((left, top, left+s, top+s))\n",
    "    except Exception:\n",
    "        w, h = pil.size; s=min(w,h); left=(w-s)//2; top=(h-s)//2\n",
    "        face = pil.crop((left, top, left+s, top+s))\n",
    "    face = face.resize(target_size)\n",
    "    face.save(out_path, format='JPEG', quality=90)\n",
    "    return True\n",
    "\n",
    "video_exts = ('.mp4', '.avi', '.mov', '.mkv', '.webm')\n",
    "count_videos = 0\n",
    "max_videos = 2000\n",
    "frames_per_video = 10\n",
    "\n",
    "for root, dirs, files in os.walk('kaggle_dataset'):\n",
    "    for fname in files:\n",
    "        if fname.lower().endswith(video_exts):\n",
    "            fpath = os.path.join(root, fname)\n",
    "            lower = fpath.lower()\n",
    "            if 'original' in lower or ('real' in lower and 'manipulated' not in lower):\n",
    "                out_dir = OUT_REAL\n",
    "            else:\n",
    "                out_dir = OUT_FAKE\n",
    "            try:\n",
    "                cap = cv2.VideoCapture(fpath)\n",
    "                if not cap.isOpened():\n",
    "                    continue\n",
    "                total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "                if total <= 0:\n",
    "                    cap.release(); continue\n",
    "                indices = np.linspace(0, max(total-1,0), frames_per_video, dtype=int)\n",
    "                for i, idx in enumerate(indices):\n",
    "                    cap.set(cv2.CAP_PROP_POS_FRAMES, int(idx))\n",
    "                    ret, frame = cap.read()\n",
    "                    if not ret:\n",
    "                        continue\n",
    "                    out_name = os.path.join(out_dir, f\"{Path(root).name}_{Path(fname).stem}_{i}.jpg\")\n",
    "                    save_face_from_frame(frame, out_name)\n",
    "                cap.release()\n",
    "                count_videos += 1\n",
    "                if count_videos % 50 == 0:\n",
    "                    print('Processed videos:', count_videos)\n",
    "                if count_videos >= max_videos:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print('Error processing', fpath, e)\n",
    "    if count_videos >= max_videos:\n",
    "        break\n",
    "\n",
    "print('Done. Extracted frames from', count_videos, 'videos.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa6654f",
   "metadata": {},
   "source": [
    "## 4 — Create ImageDataGenerators and verify counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dd0b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "data_dir = Path('data')\n",
    "generator = ImageDataGenerator(validation_split=0.2, rescale=1./255)\n",
    "\n",
    "train_gen = generator.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224,224),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    subset='training',\n",
    "    shuffle=True\n",
    ")\n",
    "val_gen = generator.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224,224),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    subset='validation',\n",
    "    shuffle=False\n",
    ")\n",
    "print('Train samples:', train_gen.samples, 'Validation samples:', val_gen.samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa92e3a",
   "metadata": {},
   "source": [
    "## 5 — Build model (Transfer learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36e148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "IMG_SHAPE = (224,224,3)\n",
    "try:\n",
    "    base_model = tf.keras.applications.EfficientNetB0(include_top=False, input_shape=IMG_SHAPE, weights='imagenet')\n",
    "    print('Using EfficientNetB0')\n",
    "except Exception:\n",
    "    base_model = tf.keras.applications.MobileNetV2(include_top=False, input_shape=IMG_SHAPE, weights='imagenet')\n",
    "    print('Using MobileNetV2')\n",
    "\n",
    "base_model.trainable = False\n",
    "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "x = base_model(inputs, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "model = models.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7876f9d",
   "metadata": {},
   "source": [
    "## 6 — Train (lightweight demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0433cd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "history = None\n",
    "if train_gen.samples > 0:\n",
    "    history = model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS)\n",
    "else:\n",
    "    print('No training samples found. Make sure the extraction step ran successfully and data/real & data/fake have images.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54266897",
   "metadata": {},
   "source": [
    "## 7 — Evaluation: Confusion Matrix & Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d495cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "if val_gen.samples > 0 and history is not None:\n",
    "    val_gen.reset()\n",
    "    preds = model.predict(val_gen, verbose=1)\n",
    "    y_pred = (preds.ravel() > 0.5).astype(int)\n",
    "    y_true = val_gen.classes\n",
    "    print('Classification report:\\n', classification_report(y_true, y_pred, digits=4))\n",
    "    print('Confusion matrix:\\n', confusion_matrix(y_true, y_pred))\n",
    "else:\n",
    "    print('Skipping evaluation — no validation samples or no history')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d43c92",
   "metadata": {},
   "source": [
    "## 8 — Grad-CAM visualization (explainability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dbc368",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf, numpy as np, matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n",
    "    grad_model = tf.keras.models.Model([model.inputs], [model.get_layer(last_conv_layer_name).output, model.output])\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_array)\n",
    "        if pred_index is None:\n",
    "            pred_index = tf.argmax(predictions[0])\n",
    "        class_channel = predictions[:, pred_index]\n",
    "    grads = tape.gradient(class_channel, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "    heatmap = tf.maximum(heatmap, 0) / (tf.math.reduce_max(heatmap) + 1e-8)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "def find_last_conv_layer(model):\n",
    "    for layer in reversed(model.layers):\n",
    "        if len(layer.output_shape) == 4:\n",
    "            return layer.name\n",
    "    raise ValueError('Could not find 4D layer.')\n",
    "\n",
    "last_conv = find_last_conv_layer(model)\n",
    "print('Last conv layer:', last_conv)\n",
    "\n",
    "def show_gradcam_on_image(img_path, model, last_conv_layer_name):\n",
    "    import cv2, numpy as np\n",
    "    img = keras_image.load_img(img_path, target_size=(224,224))\n",
    "    arr = keras_image.img_to_array(img)/255.0\n",
    "    inp = np.expand_dims(arr, axis=0)\n",
    "    preds = model.predict(inp)[0][0]\n",
    "    heatmap = make_gradcam_heatmap(inp, model, last_conv_layer_name)\n",
    "    heatmap = cv2.resize(heatmap, (224,224))\n",
    "    heatmap = np.uint8(255 * heatmap)\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
    "    img_rgb = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "    img_rgb = cv2.resize(img_rgb, (224,224))\n",
    "    superimposed = cv2.addWeighted(img_rgb, 0.6, heatmap, 0.4, 0)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.imshow(superimposed)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Pred: {preds:.4f} (0=Real,1=Fake)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19c53a1",
   "metadata": {},
   "source": [
    "## 9 — Run Grad-CAM on a validation image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e1c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "val_imgs = glob.glob('data/*/*.*')[:20]\n",
    "if len(val_imgs) == 0:\n",
    "    print('No images found under data/. Run extraction step.')\n",
    "else:\n",
    "    sample = val_imgs[0]\n",
    "    print('Sample image:', sample)\n",
    "    show_gradcam_on_image(sample, model, last_conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b000361",
   "metadata": {},
   "source": [
    "## 10 — Inference helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c290356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageOps\n",
    "def predict_image(img_path, model, threshold=0.5):\n",
    "    img = Image.open(img_path).convert('RGB').resize((224,224))\n",
    "    arr = np.array(img)/255.0\n",
    "    p = model.predict(np.expand_dims(arr, axis=0))[0][0]\n",
    "    label = 'Fake' if p > threshold else 'Real'\n",
    "    return label, float(p)\n",
    "\n",
    "if len(glob.glob('data/*/*.*'))>0:\n",
    "    print('Example prediction:', predict_image(glob.glob('data/*/*.*')[0], model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755d808d",
   "metadata": {},
   "source": [
    "## Notes & Next Steps\n",
    "- Increase `frames_per_video` and `max_videos` for more data.\n",
    "- Use Colab GPU for faster training (Runtime -> Change runtime type -> GPU).\n",
    "- For improved performance, unfreeze backbone layers and fine-tune, or use temporal models (3D CNNs / LSTM)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
